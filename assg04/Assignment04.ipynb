{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4\n",
    "_Jiajie Lu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    ">Consider the equipment replacement problem of Assignment 2.  Assume that we would like to identify the optimal replacement policy by solving an infinite-horizon discounted total reward problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.1)**\n",
    "Formulate the infinite-horizon Markov decision problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**<br>\n",
    "* State space : $\\mathcal{X}=\\{0,1,2,3,...\\}$ state of equipement; $x=0$ - new\n",
    "* Control space : $\\mathcal{U}=\\{0:\\text{\"continue\"},1:\\text{\"renew\"}\\}$\n",
    "* Transition probability\n",
    "$$\n",
    "P(x_{t+1}=k|x_t=i,0)=\\left\\{\n",
    "\\begin{aligned}\n",
    "p_{k-j} && k>i\\\\\n",
    "0 && \\text{o.w.}\n",
    "\\end{aligned}\n",
    "\\right.\\\\\n",
    "P(x_{t+1}=k|x_t=i,1)=p_k, k=0,1,\\ldots\n",
    "$$\n",
    "where $p_j=\\frac{\\lambda^j}{j!}e^{-\\lambda}$\n",
    "* Reward function :\n",
    "$$\n",
    "r(x,u)=\\left\\{\n",
    "\\begin{aligned}\n",
    "&R-K(1-\\gamma e^{-\\mu x})-c_0 && u=1\\\\\n",
    "&R-(c_0+c_1x) && u=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "* Dynamic programming equation\n",
    "$$\n",
    "v^*(x)=\\max_{u\\in\\{0,1\\}}\\{r(x,u)+\\alpha\\sum_{j=0}^{\\infty}p_jv^*[x(1-u)+j]\\}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.2)** If there is no salvage value, then show that the optimal value function is non-increasing function of the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**<br>\n",
    "The value iteration equation is just\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^{k+1}(x)&=\\max\\{r(x,0)+\\alpha\\sum_{j=0}^{\\infty}p_jv^{k}(x+j), r(x,1)+\\alpha\\sum_{j=0}^{\\infty}p_jv^{k}(j)\\}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "And\n",
    "$$\n",
    "r(x,u)=\\left\\{\n",
    "\\begin{aligned}\n",
    "&R-K-c_0 && u=1\\\\\n",
    "&R-(c_0+c_1x) && u=0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "Since $c_1>0$, the reward function is non-increasing.<br>\n",
    "Starting from $v^0(\\cdot)=0$, we have\n",
    "$\n",
    "v^1(x)=\\max\\{r(x,0), r(x,1)\\}\n",
    "$\n",
    "For $x\\ge\\frac{K}{c^{}_1}$, $v^1(x)=r(x, 1)$ which is a constant. And for $x\\le\\frac{K}{c^{}_1}$, $v^1(x)=r(x, 0)$ which is obviously nonincreasing.<br>\n",
    "Inductively, if $v^k(\\cdot)$ is non-increasing, then \n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(x)&=r(x,0)+\\alpha\\sum_{j=0}^{\\infty}p_jv^{k}(x+j)\\\\\n",
    "g(x)&=r(x,1)+\\alpha\\sum_{j=0}^{\\infty}p_jv^{k}(j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "and obviously both $h(x)$ and $g(x)$ are non-increasing. We have\n",
    "$$\n",
    "v^{k+1}(x)=\\max\\{h(x), g(x)\\}\n",
    "$$\n",
    "For $x>y$, we can verify that\n",
    "$$\n",
    "\\max\\{h(x),g(x)\\}\\le\\max\\{h(y),g(y)\\}\n",
    "$$\n",
    "Therefore we can obtain that $v^{k+1}(\\cdot)$ is non-increasing. As we know that $v^k$ converges to $v^*$ when $k\\to\\infty$, $v^*(\\cdot)$ is non-increasing conclusively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.3)** Solve the infinite horizon problem (with salvage value present) for the following values of the parameters:$c_0= 1$,$c_1= 1$,$R= 5$,$K= 10$,$\\gamma= 0.8$,$\\mu= 0.2$,$\\lambda= 1$ and discount factor $\\alpha= 0.9$.Solve the problem in value iteration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action space\n",
    "U1 = [0,1]\n",
    "# state space ; thanks to the analysis in Assignment 02\n",
    "X1 = list(range(10))\n",
    "# parameter table\n",
    "c0, c1 = 1, 1\n",
    "R, K = 5, 10\n",
    "gamma, mu, lamda = 0.8, 0.2, 1\n",
    "alpha = 0.9\n",
    "\n",
    "# reward function\n",
    "def reward(x,u):\n",
    "    if u == 0:\n",
    "        return R-c0-c1*x\n",
    "    return R-K-c0+K*gamma*np.exp(-mu*x)\n",
    "\n",
    "# probability function\n",
    "def prob(j):\n",
    "    return (lamda**j/np.math.factorial(j))*np.exp(-lamda)\n",
    "\n",
    "# show result\n",
    "def show_result_1(v, p):\n",
    "    return pd.DataFrame(\n",
    "        np.vstack([v, p]), index=[\"value\", \"policy\"], \n",
    "        columns=[f\"x={t}\" for t in X1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try the value iteration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 106 iterations, we can obtain:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x=0</th>\n",
       "      <th>x=1</th>\n",
       "      <th>x=2</th>\n",
       "      <th>x=3</th>\n",
       "      <th>x=4</th>\n",
       "      <th>x=5</th>\n",
       "      <th>x=6</th>\n",
       "      <th>x=7</th>\n",
       "      <th>x=8</th>\n",
       "      <th>x=9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>value</td>\n",
       "      <td>18.999813</td>\n",
       "      <td>16.248869</td>\n",
       "      <td>14.362373</td>\n",
       "      <td>13.390306</td>\n",
       "      <td>12.594444</td>\n",
       "      <td>11.942848</td>\n",
       "      <td>11.409366</td>\n",
       "      <td>10.972588</td>\n",
       "      <td>10.614985</td>\n",
       "      <td>10.322204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>policy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              x=0        x=1        x=2        x=3        x=4        x=5  \\\n",
       "value   18.999813  16.248869  14.362373  13.390306  12.594444  11.942848   \n",
       "policy   0.000000   0.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "              x=6        x=7        x=8        x=9  \n",
       "value   11.409366  10.972588  10.614985  10.322204  \n",
       "policy   1.000000   1.000000   1.000000   1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Value Iteration\n",
    "def value_iteration_1(T, max_iter=1000, tol=0.0001):\n",
    "    # T is the length of stricted state space\n",
    "    # initial value function\n",
    "    v = np.zeros(2*T)\n",
    "    v_old = np.zeros(2*T) + 10000\n",
    "    # inital policy path\n",
    "    p = np.zeros(2*T)\n",
    "    # current iteration\n",
    "    current_step = 0\n",
    "    \n",
    "    # value iteration method\n",
    "    while (np.linalg.norm(v - v_old) > tol) and (current_step < max_iter):\n",
    "        v_old = v.copy()\n",
    "        current_step += 1\n",
    "        \n",
    "        for x in X1:\n",
    "            vs = [reward(x, u) + alpha*np.sum([prob(j)*v_old[x*(1-u)+j] for j in X1]) for u in U1]\n",
    "            v[x], p[x] = np.max(vs), np.argmax(vs)\n",
    "           \n",
    "        for y in X1[1:]:\n",
    "             v[y+9] = reward(y+9, 1) + alpha*np.sum([prob(j)*v_old[j] for j in X1])\n",
    "            \n",
    "    return v[:T], p[:T], current_step\n",
    "\n",
    "v, p, iters = value_iteration_1(10)\n",
    "print(f\"With {iters} iterations, we can obtain:\")\n",
    "display(show_result_1(v, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    ">We consider an inventory model as discussed in class.  The stock at the beginning of period $t$ denoted by $x_t$, orders at the beginning of period $t$ by $u_t$, and random demand in period $t$ (observed only after the orders are placed) by $d_t$.  We assume ordering cost 5, selling price 10 and holding cost 2.  The demands in successive periods are i.i.d.  with values (0,1,2,3,4) whose respective probabilities are 0.1,0.2,0.3,0.2,0.2.  The capacity of the inventory is 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.1)**\n",
    "Formulate an infinite horizon problem with discount factor 0.8 to determine the best re-order policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* State space : $\\mathcal{X}=\\{0,1,2,3,...,12\\}$ - state of the equipment; $x=0$ - new\n",
    "* Control space : $\\mathcal{U}=\\{0,1,2,3,...,12\\}$\n",
    "* Dynamics : $x_{t+1}=\\max\\{0, x_t+u_t-d_t\\}$\n",
    "* Control mapping : $U(x)=\\{0,1,2,3,4,5,...,12-x\\}$\n",
    "* Reward function\n",
    "$$\n",
    "r(x,u)=E[10\\min(d, x+u) - 5u - 2(x+u))]\n",
    "$$\n",
    "* Dynamic programming equation :\n",
    "$$\n",
    "v^*(x)=\\max_{u\\in\\mathcal{U(x)}}\\{-5u-2(x+u)+\\sum_{d=0}^{4}P(d)(10\\min\\{x+u,d\\}+0.8v^*(\\max\\{0, x+u-d\\}))\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.2)** Solve the problem in (2.1) by value and policy iteration methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# state space\n",
    "X2 = list(range(13))\n",
    "# number of demand\n",
    "D = 5\n",
    "# control space\n",
    "U2 = list(range(13))\n",
    "# probability of demand\n",
    "P = [0.1, 0.2, 0.3, 0.2, 0.2]\n",
    "# discount factor\n",
    "alpha2 = 0.8\n",
    "\n",
    "# show result\n",
    "def show_result_2(v, p):\n",
    "    return pd.DataFrame(\n",
    "        np.vstack([v, p]), index=[\"value\", \"policy\"], \n",
    "        columns=[f\"x={t}\" for t in X2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to do with value iteration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 65 iterations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x=0</th>\n",
       "      <th>x=1</th>\n",
       "      <th>x=2</th>\n",
       "      <th>x=3</th>\n",
       "      <th>x=4</th>\n",
       "      <th>x=5</th>\n",
       "      <th>x=6</th>\n",
       "      <th>x=7</th>\n",
       "      <th>x=8</th>\n",
       "      <th>x=9</th>\n",
       "      <th>x=10</th>\n",
       "      <th>x=11</th>\n",
       "      <th>x=12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>value</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.348</td>\n",
       "      <td>35.278</td>\n",
       "      <td>36.487</td>\n",
       "      <td>36.913</td>\n",
       "      <td>36.395</td>\n",
       "      <td>34.962</td>\n",
       "      <td>32.688</td>\n",
       "      <td>29.728</td>\n",
       "      <td>26.107</td>\n",
       "      <td>21.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>policy</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x=0   x=1   x=2     x=3     x=4     x=5     x=6     x=7     x=8  \\\n",
       "value   18.0  23.0  28.0  32.348  35.278  36.487  36.913  36.395  34.962   \n",
       "policy   2.0   1.0   0.0   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "\n",
       "           x=9    x=10    x=11    x=12  \n",
       "value   32.688  29.728  26.107  21.887  \n",
       "policy   0.000   0.000   0.000   0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# value iteration\n",
    "def value_iteration_2(k, max_iter=1000, tol=1e-5):\n",
    "    # Kk- the capacity of Inventory\n",
    "    # initial value functions\n",
    "    v = np.zeros(len(X2))\n",
    "    v_old = np.zeros(len(X2)) + 100\n",
    "    # initial policy\n",
    "    p = np.zeros(len(X2))\n",
    "    # current iter step\n",
    "    current_step = 0\n",
    "    \n",
    "    while (np.linalg.norm(v - v_old) > tol) and (current_step < max_iter):\n",
    "        v_old = v.copy()\n",
    "        current_step += 1\n",
    "        \n",
    "        for x in X2:\n",
    "            tmp = [-5*u - 2*(x+u) + np.sum([\n",
    "                P[d]*(10*np.min([x+u, d]) + alpha2*v_old[int(np.max([0, x+u-d]))]) \n",
    "                for d in range(D)]) \n",
    "                   for u in range(k-x+1)]\n",
    "            v[x], p[x] = np.max(tmp), np.argmax(tmp)\n",
    "            \n",
    "    return v, p, current_step\n",
    "\n",
    "v, p, iters = value_iteration_2(k=12)\n",
    "print(f'With {iters} iterations:')\n",
    "display(show_result_2(np.round(v, 3), p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 2 iterations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x=0</th>\n",
       "      <th>x=1</th>\n",
       "      <th>x=2</th>\n",
       "      <th>x=3</th>\n",
       "      <th>x=4</th>\n",
       "      <th>x=5</th>\n",
       "      <th>x=6</th>\n",
       "      <th>x=7</th>\n",
       "      <th>x=8</th>\n",
       "      <th>x=9</th>\n",
       "      <th>x=10</th>\n",
       "      <th>x=11</th>\n",
       "      <th>x=12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>value</td>\n",
       "      <td>22.755</td>\n",
       "      <td>27.755</td>\n",
       "      <td>32.755</td>\n",
       "      <td>36.205</td>\n",
       "      <td>38.843</td>\n",
       "      <td>39.767</td>\n",
       "      <td>39.912</td>\n",
       "      <td>39.063</td>\n",
       "      <td>37.398</td>\n",
       "      <td>34.899</td>\n",
       "      <td>31.734</td>\n",
       "      <td>27.921</td>\n",
       "      <td>23.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>policy</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x=0     x=1     x=2     x=3     x=4     x=5     x=6     x=7  \\\n",
       "value   22.755  27.755  32.755  36.205  38.843  39.767  39.912  39.063   \n",
       "policy   2.000   1.000   0.000   0.000   0.000   0.000   0.000   0.000   \n",
       "\n",
       "           x=8     x=9    x=10    x=11    x=12  \n",
       "value   37.398  34.899  31.734  27.921  23.534  \n",
       "policy   0.000   0.000   0.000   0.000   0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "def policy_iteration_2(k, max_iter=1000, tol=1e-5):\n",
    "    # k - capacity of the inventory\n",
    "    # initial policy\n",
    "    p = np.zeros(len(X2))\n",
    "    p_old = np.zeros(len(X2)) + 10\n",
    "    # current step\n",
    "    current_step = 0\n",
    "    \n",
    "    while (np.linalg.norm(p - p_old) > tol) and (current_step < max_iter):\n",
    "        p_old = p.copy()\n",
    "        current_step += 1\n",
    "        \n",
    "        # initial coef\n",
    "        A = np.zeros((len(X2), len(X2)))\n",
    "        # initial bias\n",
    "        b = np.zeros(len(X2))\n",
    "        # set coef&bias\n",
    "        for x in X2:\n",
    "            u = p_old[x]\n",
    "            A[x, x] += 1\n",
    "            b[x] += -5*u - 2*(x+u) + np.sum([\n",
    "                P[d]*(10*np.min([x+u, d])) for d in range(D)\n",
    "            ])\n",
    "            \n",
    "            for d in range(D):\n",
    "                A[x, int(np.max(x+u-d, 0))] += -alpha2*P[d]\n",
    "                \n",
    "        # iterated v\n",
    "        v = np.linalg.solve(A, b)\n",
    "        # update policy\n",
    "        for x in X2:\n",
    "            tmp = [-5*u - 2*(x+u) + np.sum([\n",
    "                P[d]*(10*np.min([x+u, d]) + alpha2*v[int(np.max([0, x+u-d]))])\n",
    "                for d in range(D)\n",
    "            ]) for u in range(k-x+1)]\n",
    "            p[x] = np.argmax(tmp)\n",
    "            \n",
    "    return v, p, current_step\n",
    "\n",
    "v, p, iters = policy_iteration_2(12)\n",
    "print(f\"With {iters} iterations:\")\n",
    "display(show_result_2(np.round(v,3), p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    ">Fisher boat is sent to the waters of three connected lakes during one fishing season. Let $x_i$, $i= 1,2,3$ be the (estimated) current amounts of fish in lake $i$.  If we fish in lake $i$, then we harvest $r_ix_i$ fish, provided the fishing conditions are good.  The weather may change abruptly with probability $p$ so that we end the fishing season.  We assume that $0< r_i<1$ for all $i= 1,2,3$.  Identify the lake-selection policy that maximizes the amount of fish before the end of the season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* State space : $\\mathcal{X}=\\{x=(x_1,x_2,x_3)\\in\\mathbb{R}_+^3\\}$\n",
    "* Control space : $\\mathcal{U}=\\{1,2,3\\}$\n",
    "* Transition probability :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P[(0,0,0)|(x_1, x_2, x_3), u=0]&=p\\\\\n",
    "P[((1-r_1)x_1, x_2, x_3)|(x_1, x_2, x_3), u=1]&=1-p\\\\\n",
    "P[(x_1, (1-r_2)x_2, x_3)|(x_1, x_2, x_3), u=2]&=1-p\\\\\n",
    "P[(x_1, x_2, (1-r_3)x_3)|(x_1, x_2, x_3), u=3]&=1-p\n",
    "\\end{aligned}\n",
    "$$\n",
    "$(0, 0, 0)$ represents the season ends.\n",
    "* Reward function :\n",
    "$$\n",
    "r(x, u) =\\left\\{\n",
    "\\begin{aligned}\n",
    "0 && u=0\\\\\n",
    "(1-p)r_ux_u && u\\in\\{1,2,3\\}\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "* State equation : \n",
    "$$\n",
    "x^{t+1}=x^t_{-u}\n",
    "$$\n",
    "The subscript $-u$ means the u-th lake was havested and the u-th item of $x^{t+1}$ is $(1-r_u)x^t_u$.\n",
    "* Dynamic programming equation :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^*(x)=\\max\\{0, \\max_{u\\in U}\\{(1-p)r_ux_u+(1-p)v^*(x_{-u})\\}\\}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic programming problem here can be considered as a 3-armed bandit problem. And the optimal policy is just\n",
    "$$\n",
    "\\pi^*(x)=\\arg\\max_{u\\in\\mathcal{U}}r_ux_u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows if the weather permits, we just choose the lake which we can harvest highest amount of fish."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
