{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 5\n",
    "_Jiajie Lu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    ">Consider the chess game from Assignment 2. Assume that if you play timid your probability of making a draw is $p = 0.8$ and the probability to win is the same as the probability to lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Solve the problem again with this new probability setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* Transition Probability\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "P(x_{t+1}=x_t+0|x_t,u=0)=0.8, &P(x_{t+1}=x_t-1|x_t,u=0)=0.1, &P(x_{t+1}=x_t+1|x_t,u=0)=0.1,\\\\\n",
    "P(x_{t+1}=x_t+0|x_t,u=1)=0, &P(x_{t+1}=x_t-1|x_t,u=1)=0.55, &P(x_{t+1}=x_t+1|x_t,u=1)=0.45.\n",
    "\\end{array}\n",
    "$$\n",
    "In Assignment 2, if we step into the \"sudden death\" phase, we have no choice but to play bold, because we cannot win if playing timid. However here we cannot simply play bold since the probability of playing timid has modified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we cannot simply set our $v_6(\\cdot)$ as before. Then we can treat it as an infinite horizon dynamic programming problem of which any state except $0$ is absorbing. That means we need to find out the optimal value function and optimal policy for the \"sudden death\" phase. Then we set up as following\n",
    "* State Space:\n",
    "$$\n",
    "\\mathcal{S}=\\{-1,0,1\\}\n",
    "$$\n",
    "representing \"lose\", \"draw\" and \"win\" in \"sudden death\" phase.\n",
    "* Control Space:\n",
    "$$\n",
    "\\{0:\\text{timid}, 1:\\text{bold}\\}\n",
    "$$\n",
    "* Transition Probability:\n",
    "$$\n",
    "p^{0}_{0,\\cdot}=\\begin{bmatrix}\n",
    "0.1&0.8&0.1\n",
    "\\end{bmatrix},\\qquad \n",
    "p^{1}_{0,\\cdot}=\\begin{bmatrix}\n",
    "0.55&0&0.45\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "* Dynamic Programming Equation:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "&v_6^*(0)=\\max\\{\\sum_{s\\in\\mathcal{S}}p^0_{0,s}v_6^*(s), \\sum_{s\\in\\mathcal{S}}p^1_{0,s}v_6^*(s)\\},\\\\\n",
    "&v_6^*(1)=1,\\quad v_6^*(-1)=0\n",
    "\\end{array}\n",
    "$$\n",
    "Here we denote the optimal value function of the previous $v_6(\\cdot)$ by $v_6^*(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "h&=&\\sum_{s\\in\\mathcal{S}}p^0_{0,s}v_6^*(s)\\\\\n",
    "&=&0.1\\cdot0+0.8\\cdot v_6^*(0)+0.1\\cdot1\\\\\n",
    "&=&0.8v_6^*(0)+0.1\n",
    "\\end{array}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "g&=&\\sum_{s\\in\\mathcal{S}}p^1_{0,s}v_6^*(s)\\\\\n",
    "&=&0.55\\cdot0+0\\cdot v_6^*(0)+0.45\\cdot1\\\\\n",
    "&=&0.45\n",
    "\\end{array}\n",
    "$$\n",
    "If we play timid at state $s=0$, then we have\n",
    "$$\n",
    "h=0.8v_6^*(0)+0.1=v_6^*(0)\\implies v_6^*(0)=0.5.\n",
    "$$\n",
    "If we play bold at state $s=0$, then we have\n",
    "$$\n",
    "g=0.45=v_6^*(0)<0.5.\n",
    "$$\n",
    "That shows playing timid is the optimal policy for \"sudden death\". Then we can set up\n",
    "$$\n",
    "v_6(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1, &x>0,\\\\\n",
    "0.5, &x=0,\\\\\n",
    "0, &x<0.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "Then solve the problem as we did previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Investigate how your strategy changes when the probability of wining changes in the case of a timid play. The probability of wining can change from 0 to 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p$ be the probability of winning when playing timid. With our preceeding notation, we can obtain\n",
    "$$\n",
    "h=(0.2-p)\\cdot0+0.8v_6^*(0)+p\\cdot1=0.8v_6^*(0)+p,\n",
    "$$\n",
    "and $g=0.45$.\n",
    "If playing timid at $s=0$, then we can obtain\n",
    "$$\n",
    "h=0.8v_6^*(0)+p=v_6^*(0)\\implies  v_6^*(0)=5p.\n",
    "$$\n",
    "and it's clear that $v_6^*(0)=0.45$ when playing bold at $s=0$.\n",
    "Then if \n",
    "$$\n",
    "5p\\ge 0.45\\implies p\\ge0.09,\n",
    "$$\n",
    "the optimal policy is just playing timid and playing bold otherwise.\n",
    "Then based on the constraint that $0\\le p\\le 0.15$, we have that the optimal policy $\\pi^*$ of $v_6(\\cdot)$ is just\n",
    "$$\n",
    "\\pi^*=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\text{timid}, &0.09\\le p\\le 0.15,\\\\\n",
    "\\text{bold}, &0\\le p\\le 0.09.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    ">We have a tree farm. At any time, the size $s$ of a tree is 0,1,2,3,4, where 0 means that the tree has died, and 4 is the size of a mature tree. We need to decide when to harvest a given tree. Each year it costs about &#36;10+s to maintain a tree, and 30+5s to harvest a tree. The sales price of a tree of each size is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Tree Size     | 1 |             2                | 3 | 4|\n",
    "| ------------ | --- | ------------------------------- | ------------------------------- | ------------------------------- |\n",
    "| Sale Prize |  150 | 180 | 210 | 260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The transition probability matrix for the size of the tree is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| sizes | 0 | 1 |  2 |  3 |   4 |\n",
    "| :--: |:--:|:--:|:--:|:--:|:--:|\n",
    "|0| 1| 0 |   0 |  0 |   0 |\n",
    "|1| 0.05|  0.15 | 0.7 | 0.1 |  0|\n",
    "|2|0.05 |  0 |  0.2 | 0.7 | 0.05|\n",
    "|3|0.05 |  0 |   0 |  0.5 | 0.45|\n",
    "|4|0.05 |  0 |   0 |  0 |  0.95|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Describe a dynamic programming problem to determine an optimal harvesting policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* State space : $\\{0,1,2,3,4\\}$\n",
    "* Control space : $\\{0:\\text{maintain},1:\\text{harvest}\\}$\n",
    "* Transition probability : \n",
    "$$\n",
    "p^1_{ij}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1, &j=0,\\\\\n",
    "0, &j\\ne0.\n",
    "\\end{array}\n",
    "\\right.,\\qquad u=1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "p^0_{ij}=P_{ij},\\qquad u=0,\n",
    "$$\n",
    "where $P$ is the given transition probaility matrix.\n",
    "* reward function :\n",
    "$$\n",
    "r(s,u)=\\left\\{\n",
    "\\begin{aligned}\n",
    "-10-s && u=0\\\\\n",
    "-30-5s+K_s && u=1\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "where $K_s$ is the sale price of tree with size $x$.\n",
    "* Dynamic programming equation :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^*(s)&=\\max\\{r(s,0)+\\sum_{i=0}^{4}P_{si}v^*(i),r(s,1)\\},x=1,2,3,4,\\\\\n",
    "v^*(0)&=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Solve the problem numerically. What numerical methods are applicable to this problem and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**<br>\n",
    "The state space is finite. We can use value iteration and linear programming. Here I just apply value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transtion prob mat\n",
    "P = np.array(\n",
    "    [\n",
    "        [1,  0,  0,  0,  0],\n",
    "        [0.05, 0.15, 0.7, 0.1, 0],\n",
    "        [0.05, 0, 0.2, 0.7, 0.05],\n",
    "        [0.05, 0, 0, 0.5, 0.45],\n",
    "        [0.05, 0, 0, 0, 0.95]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# sale price mapping\n",
    "K = {1:150, 2:180, 3:210, 4:260}\n",
    "\n",
    "# reward function\n",
    "def reward(s,u):\n",
    "    if u == 0:\n",
    "        return -10-s\n",
    "    if u == 1:\n",
    "        return -30-5*s+K[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within 10 iterations,\n",
      "We got optimal value function:\n",
      "[  0.         123.8235125  142.49999872 165.         210.        ]\n",
      "And optimal policy\n",
      "[0. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration_tree(T, max_iter=1000, tol=0.0001):\n",
    "    # T is length of state space\n",
    "    # init value function list\n",
    "    v = np.zeros(T)\n",
    "    v_old = np.zeros(T) + 10000\n",
    "    # init policy path\n",
    "    p = np.zeros(T)\n",
    "    \n",
    "    current_step = 0\n",
    "    \n",
    "    # value iteration method\n",
    "    while (np.linalg.norm(v - v_old) > tol) and (current_step < max_iter):\n",
    "        v_old = v.copy()\n",
    "        current_step += 1\n",
    "        \n",
    "        for s in range(1, T):\n",
    "            v0 = reward(s,0) + np.sum([\n",
    "                P[s, i]*v_old[i] for i in range(T)\n",
    "            ])\n",
    "            v1 = reward(s, 1)\n",
    "            v[s], p[s] = np.max([v0, v1]), np.argmax([v0, v1])\n",
    "            \n",
    "    return v, p, current_step\n",
    "\n",
    "v, p, iters = value_iteration_tree(5)\n",
    "print(f\"Within {iters} iterations,\")\n",
    "print(\"We got optimal value function:\")\n",
    "print(v)\n",
    "print(\"And optimal policy\")\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
